<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>What is GAN and WGAN | rdh1115</title>
<meta name="keywords" content="Generative Models, Literature Review">
<meta name="description" content="A brief introduction to the mathematical concepts behind GAN and WGAN">
<meta name="author" content="Mark Bai">
<link rel="canonical" href="https://rdh1115.github.io/posts/literature/gan-wgan/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fdc284e8bb37d457cb8ad90a4d6521985678fbc9839210c5c4e08ea1f6ea9c37.css" integrity="sha256-/cKE6Ls31FfLitkKTWUhmFZ4&#43;8mDkhDFxOCOofbqnDc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://rdh1115.github.io/icons/brain_128.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://rdh1115.github.io/icons/brain_16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://rdh1115.github.io/icons/brain_32.png">
<link rel="apple-touch-icon" href="https://rdh1115.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://rdh1115.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://rdh1115.github.io/posts/literature/gan-wgan/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.0/es5/latest?tex-mml-chtml.js">
</script>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    tags: 'ams'
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
<meta property="og:url" content="https://rdh1115.github.io/posts/literature/gan-wgan/">
  <meta property="og:site_name" content="rdh1115">
  <meta property="og:title" content="What is GAN and WGAN">
  <meta property="og:description" content="A brief introduction to the mathematical concepts behind GAN and WGAN">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-12-17T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-12-17T00:00:00+00:00">
    <meta property="article:tag" content="Generative Models">
    <meta property="article:tag" content="Literature Review">
      <meta property="og:image" content="https://rdh1115.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://rdh1115.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="What is GAN and WGAN">
<meta name="twitter:description" content="A brief introduction to the mathematical concepts behind GAN and WGAN">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://rdh1115.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Literature",
      "item": "https://rdh1115.github.io/posts/literature/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "What is GAN and WGAN",
      "item": "https://rdh1115.github.io/posts/literature/gan-wgan/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "What is GAN and WGAN",
  "name": "What is GAN and WGAN",
  "description": "A brief introduction to the mathematical concepts behind GAN and WGAN",
  "keywords": [
    "Generative Models", "Literature Review"
  ],
  "articleBody": "This post explores the theory behind one of the most influential machine learning papers: Generative Adversarial Networks (GANs). Contrary to other deep learning models, I find that generative models are supported by more rigorous mathematics that are easily digestible.\nAs we understand the theory behind GANs, we will realize its limitaitons, which leads us to Wasserstein Generative Adversarial Networks (WGANs), which apply useful concepts from Optimal Transport (OT).\nSince these papers are quite “old” by machine learning standards, there are many blog posts that already discuss them. The following is my own understanding that I find missing in other posts. A Kaggle notebook is also provided to show the algorithms learning on a toy example.\nGANs Cool History The story is that GAN was conceived at a bar in Montréal (Les 3 Brasseurs) by Ian Goodfellow during his Ph.D. studies. At the bar, he proposed GAN to his friends, but quickly faced skepticism.\nThe idea of jointly training a pair of networks against each other seemed too difficult when training just one network was difficult enough. Perhaps due to the influence of alcohol, he was still confident, so he headed home, coded up GAN, and produced amazing results on MNIST (a handwritten digit recognition dataset).\nAs of today, the seminal paper published in NeurIPS has over 75,000 citations, and has become a cornerstone in the field of unsupervised learning.\nGAN Architecture GANs learn the latent distribution of the unlabeled data through a pair of networks (the generator and the discriminator) that are competing against each other. An analogy is used in the original paper: the generator is a team of counterfeiters that produces fake currency, while the discriminator is the police trying to detect forgery. When both networks are optimal, the generator is able to produce counterfeit currency that is indistinguishable from the real ones.\nFigure 1: A GAN consists of two components: the discriminator $D$ outputs the probability that a given sample is real, and the generator $G$ produces synthetic samples given a latent variable $z$ sampled from the base distribution, e.g. $z \\sim \\mathcal{N}(0,1)$. The discriminator parameters $\\theta_d$ are updated to assigns high probability to real samples and low probability to synthetic samples, while the generator parameters $\\theta_g$ are updated to \"fool\" the discriminator into assigning high probabilities to the synthetic samples. First, let’s define $p_z, p_g, p_r$ as the probability distributions of the latent variable $z$, the generator, and real samples respectively.\nThe discriminator $D(x,\\theta_d)\\to [0, 1]$ outputs a scalar that represents the probability that $x$ came from $p_r$ rather than $p_g$. The generator $G(z, \\theta_g)$ then learns a mapping from $z$ to the data space. In other words, it is learning $p_g$ over data $x$, where $x\\sim p_r(x)$.\n$D$ is trained in a binary classification fashion. It maximizes the probability of assigning the correct label to both real samples $x$ and fake samples $x’ = G(z)$. This is equivalent to maximizing $$\\mathbb{E}_{x\\sim p_r(x)}\\left[\\log (D(x))\\right] +\\mathbb{E}_{z\\sim p_z(z)}\\left[\\log(1- D(G(z)))\\right].$$\nThe generator then minimizes the probability of the discriminator assigning the correct label to the fake samples: $$\\mathbb{E}_{z\\sim p_z(z)}\\left[\\log(1- D(G(z)))\\right].$$ In other words, they are playing a MinMax game with the following loss function: $$ \\begin{equation}\\label{eq: gan_loss} \\min_{G}\\max_{D} L(G, D) = \\mathbb{E}_{x\\sim p_r(x)}\\left[\\log (D(x))\\right] + \\mathbb{E}_{z\\sim p_z(z)}\\left[\\log(1- D(G(z)))\\right] \\end{equation} $$\nHow to train GANs Since the discriminator and the generator are playing a two-player non-cooperative game, training them can be difficult, and similar to finding the Nash equilibrium (Salimans et al., 2016).\nIn practice, this is traditionally done with the following pipeline: in each training loop, fix the generator and update the discriminator for $k$ steps, then fix the discriminator and update the generator for 1 step.\nTo update the discriminator, we fix the generator and isolate the $\\max_{D} L(G, D)$ part of Equation $\\eqref{eq: gan_loss}$: $$ \\begin{aligned} L(\\theta_d) \u0026 = \\mathbb{E}_{x\\sim p_r(x)}\\left[\\log (D(x, \\theta_d))\\right] + \\mathbb{E}_{z\\sim p_z(z)}\\left[\\log(1- D(G^*(z), \\theta_d))\\right] \\\\ \u0026 \\approx \\frac{1}{m}\\sum_{i=1}^m\\left[\\log (D(x_i, \\theta_d))\\right] + \\frac{1}{n}\\sum_{j=1}^n\\left[ \\log( 1 - D ( G^* (z_j), \\theta_d)) \\right], \\end{aligned} $$ where $m$ represents the number of real examples, $n$ represents the number of fake examples, and $G^*(z)$ represents the fixed generator.\nTo update the generator, we use the following loss function: $$ \\begin{aligned} L(\\theta_g) \u0026 = \\left(\\mathbb{E}_{x\\sim p_r(x)}\\left[\\log (D^*(x))\\right] + \\mathbb{E}_{z\\sim p_z(z)}\\left[\\log(1- D^*(G(z, \\theta_g)))\\right]\\right) \\\\ \u0026 \\approx \\frac{1}{n}\\sum_{j=1}^n\\left[ \\log ( 1 - D^* (G(z_j, \\theta_g)))\\right] \\end{aligned}. $$ Note that we remove the first term since it is not related to $G(z, \\theta_g)$.\nThe Optimal Discriminator and Generator It is intuitive that in theory, the perfect generator replicates the entire data space, in other words, $p_g = p_r$ (see proof below. It is natural then to think that the perfect discriminator can always differentiate between real and synthetic samples, however in theory, the best discriminator achieves random guessing.\nLil’Log provides a great breakdown on the math behind this. The general idea is that if we take the derivative of the discriminator’s loss function with a fixed generator, and set it to zero, $D^*(x) = \\frac{p_r(x)}{p_r(x)+p_g(x)}$. And since the perfect generator enables $p_g = p_r$, $D^*(x) = \\frac{1}{2}$.\nGiven $G^*(z, \\theta_g)$ such that $p_r = p_g$, and $D^*(x, \\theta_d)$ such that $D^*(x) = \\frac{1}{2}$, we can derive the global minimum of the loss function: $$ \\begin{aligned} L(G^*, D^*) \u0026= \\mathbb{E}_{x\\sim p_r(x)}\\left[-\\log2\\right] +\\mathbb{E}_{x\\sim p_g(x)}\\left[-\\log2)\\right]\\\\ \u0026= -2\\log2 \\end{aligned} $$ Thus, the best possible value of $L(G, D)$ is $-2\\log2$.\nTangent on KL \u0026 JS Divergence ",
  "wordCount" : "891",
  "inLanguage": "en",
  "image": "https://rdh1115.github.io/images/papermod-cover.png","datePublished": "2024-12-17T00:00:00Z",
  "dateModified": "2024-12-17T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Mark Bai"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://rdh1115.github.io/posts/literature/gan-wgan/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "rdh1115",
    "logo": {
      "@type": "ImageObject",
      "url": "https://rdh1115.github.io/icons/brain_128.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://rdh1115.github.io/" accesskey="h" title="rdh1115 (Alt + H)">rdh1115</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://rdh1115.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://rdh1115.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://rdh1115.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://rdh1115.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://rdh1115.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://rdh1115.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://rdh1115.github.io/posts/">Posts</a>&nbsp;»&nbsp;<a href="https://rdh1115.github.io/posts/literature/">Literature</a></div>
    <h1 class="post-title entry-hint-parent">
      What is GAN and WGAN
    </h1>
    <div class="post-description">
      A brief introduction to the mathematical concepts behind GAN and WGAN
    </div>
    <div class="post-meta"><span title='2024-12-17 00:00:00 +0000 UTC'>December 17, 2024</span>&nbsp;·&nbsp;Estimated Reading Time: 30 min&nbsp;·&nbsp;Mark Bai

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#gans" aria-label="GANs">GANs</a><ul>
                        
                <li>
                    <a href="#cool-history" aria-label="Cool History">Cool History</a></li>
                <li>
                    <a href="#gan-architecture" aria-label="GAN Architecture">GAN Architecture</a></li>
                <li>
                    <a href="#how-to-train-gans" aria-label="How to train GANs">How to train GANs</a><ul>
                        
                <li>
                    <a href="#the-optimal-discriminator-and-generator" aria-label="The Optimal Discriminator and Generator">The Optimal Discriminator and Generator</a></li></ul>
                </li>
                <li>
                    <a href="#tangent-on-kl--js-divergence" aria-label="Tangent on KL &amp; JS Divergence">Tangent on KL &amp; JS Divergence</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>This post explores the theory behind one of the most influential machine learning papers:
<a href="https://arxiv.org/pdf/1406.2661">Generative Adversarial Networks</a> (GANs).
Contrary to other deep learning models, I find that generative models are supported by more rigorous mathematics that are easily digestible.</p>
<p>As we understand the theory behind GANs, we will realize its limitaitons, which leads us to
<a href="https://arxiv.org/pdf/1701.07875">Wasserstein Generative Adversarial Networks</a> (WGANs),
which apply useful concepts from <a href="https://en.wikipedia.org/wiki/Transportation_theory_%28mathematics%29">Optimal Transport</a> (OT).</p>
<p>Since these papers are quite &ldquo;old&rdquo; by machine learning standards, there are many blog posts that already discuss them. The following is my own understanding that I find missing in other posts.
A Kaggle notebook is also provided to show the algorithms learning on a toy example.</p>
<h2 id="gans">GANs<a hidden class="anchor" aria-hidden="true" href="#gans">#</a></h2>
<h3 id="cool-history">Cool History<a hidden class="anchor" aria-hidden="true" href="#cool-history">#</a></h3>
<p>The story is that GAN was conceived at a bar in Montréal (Les 3 Brasseurs) by Ian Goodfellow during his Ph.D. studies. At the bar, he proposed GAN to his friends, but quickly faced skepticism.</p>
<p>The idea of jointly training a pair of networks against each other seemed too difficult when training just one network was difficult enough.
Perhaps due to the influence of alcohol, he was still confident, so he headed home, coded up GAN,
and produced amazing results on <a href="https://yann.lecun.com/exdb/mnist/">MNIST</a> (a handwritten digit recognition dataset).</p>
<p>As of today, the seminal paper published in NeurIPS has over 75,000 citations, and has become a cornerstone in the field of unsupervised learning.</p>
<h3 id="gan-architecture">GAN Architecture<a hidden class="anchor" aria-hidden="true" href="#gan-architecture">#</a></h3>
<p>GANs learn the latent distribution of the unlabeled data through a pair of networks
(the generator and the discriminator) that are competing against each other.
An analogy is used in the original paper: the generator is a team of counterfeiters that produces fake currency,
while the discriminator is the police trying to detect forgery.
When both networks are optimal, the generator is able to produce counterfeit currency that is indistinguishable from the
real ones.</p>
<figure style="text-align: center;">
    <img src="/images/adversarial_net.png" alt="Alt text for the image" style="display: block; margin: 0 auto;" width="600">
    <figcaption style="text-align: left; margin-top: 0.5rem;">Figure 1: A GAN consists of two components: 
    the discriminator $D$ outputs the probability that a given sample is real, 
    and the generator $G$ produces synthetic samples given a latent variable $z$ sampled from the base distribution, e.g. $z \sim \mathcal{N}(0,1)$. 
    The discriminator parameters $\theta_d$ are updated to assigns high probability to real samples and low probability 
    to synthetic samples, while the generator parameters $\theta_g$ are updated to "fool" the discriminator into assigning high 
    probabilities to the synthetic samples.</figcaption>
</figure>
<p>First, let&rsquo;s define $p_z, p_g, p_r$ as the probability distributions of the
latent variable $z$, the generator, and real samples respectively.</p>
<p>The discriminator $D(x,\theta_d)\to [0, 1]$ outputs a scalar that represents the probability that $x$ came from $p_r$ rather than $p_g$.
The generator $G(z, \theta_g)$ then learns a mapping from $z$ to the data space.
In other words, it is learning $p_g$ over data $x$, where $x\sim p_r(x)$.</p>
<p>$D$ is trained in a binary classification fashion. It maximizes the probability of assigning the correct label to both
real samples $x$ and fake samples $x&rsquo; = G(z)$.
This is equivalent to maximizing $$\mathbb{E}_{x\sim p_r(x)}\left[\log (D(x))\right] +\mathbb{E}_{z\sim p_z(z)}\left[\log(1- D(G(z)))\right].$$</p>
<p>The generator then minimizes the probability of the discriminator assigning the correct label to the fake samples:
$$\mathbb{E}_{z\sim p_z(z)}\left[\log(1- D(G(z)))\right].$$
In other words, they are playing a MinMax game with the following loss function:
$$
\begin{equation}\label{eq: gan_loss}
\min_{G}\max_{D} L(G, D) =
\mathbb{E}_{x\sim p_r(x)}\left[\log (D(x))\right] +
\mathbb{E}_{z\sim p_z(z)}\left[\log(1- D(G(z)))\right]
\end{equation}
$$</p>
<h3 id="how-to-train-gans">How to train GANs<a hidden class="anchor" aria-hidden="true" href="#how-to-train-gans">#</a></h3>
<p>Since the discriminator and the generator are playing a two-player non-cooperative game,
training them can be difficult, and similar to finding the Nash equilibrium (<a href="https://arxiv.org/pdf/1606.03498">Salimans et al., 2016</a>).</p>
<p>In practice, this is traditionally done with the following pipeline: in each training loop, fix the generator and update the discriminator for $k$ steps,
then fix the discriminator and update the generator for 1 step.</p>
<p>To update the discriminator, we fix the generator and isolate the $\max_{D} L(G, D)$ part of Equation $\eqref{eq: gan_loss}$:
$$
\begin{aligned}
L(\theta_d)
&amp; = \mathbb{E}_{x\sim p_r(x)}\left[\log (D(x, \theta_d))\right] + \mathbb{E}_{z\sim p_z(z)}\left[\log(1- D(G^*(z), \theta_d))\right] \\
&amp; \approx \frac{1}{m}\sum_{i=1}^m\left[\log (D(x_i, \theta_d))\right] + \frac{1}{n}\sum_{j=1}^n\left[ \log( 1 - D ( G^* (z_j), \theta_d)) \right],
\end{aligned}
$$
where $m$ represents the number of real examples, $n$ represents the number of fake examples, and $G^*(z)$ represents the fixed generator.<br>
To update the generator, we use the following loss function:
$$
\begin{aligned}
L(\theta_g)
&amp; = \left(\mathbb{E}_{x\sim p_r(x)}\left[\log (D^*(x))\right] + \mathbb{E}_{z\sim p_z(z)}\left[\log(1- D^*(G(z, \theta_g)))\right]\right) \\
&amp; \approx \frac{1}{n}\sum_{j=1}^n\left[ \log ( 1 - D^* (G(z_j, \theta_g)))\right]
\end{aligned}.
$$
Note that we remove the first term since it is not related to $G(z, \theta_g)$.</p>
<h4 id="the-optimal-discriminator-and-generator">The Optimal Discriminator and Generator<a hidden class="anchor" aria-hidden="true" href="#the-optimal-discriminator-and-generator">#</a></h4>
<p>It is intuitive that in theory, the perfect generator replicates the entire data space,
in other words, $p_g = p_r$ (see proof <a href="/posts/literature/gan-wgan/#tangent-on-kl--js-divergence">below</a>. It is natural then to think that the perfect discriminator can always differentiate between real and synthetic samples, however
in theory, the best discriminator achieves random guessing.</p>
<p>Lil&rsquo;Log provides a great breakdown on the <a href="https://lilianweng.github.io/posts/2017-08-20-gan/#what-is-the-optimal-value-for-d">math behind this</a>.
The general idea is that if we take the derivative of the discriminator&rsquo;s loss function with a fixed generator, and set it to zero,
$D^*(x) = \frac{p_r(x)}{p_r(x)+p_g(x)}$. And since the perfect generator enables $p_g = p_r$,
$D^*(x) = \frac{1}{2}$.</p>
<p>Given $G^*(z, \theta_g)$ such that $p_r = p_g$, and $D^*(x, \theta_d)$ such that $D^*(x) = \frac{1}{2}$, we can
derive the global minimum of the loss function:
$$
\begin{aligned}
L(G^*, D^*)
&amp;= \mathbb{E}_{x\sim p_r(x)}\left[-\log2\right] +\mathbb{E}_{x\sim p_g(x)}\left[-\log2)\right]\\
&amp;= -2\log2
\end{aligned}
$$
Thus, the best possible value of $L(G, D)$ is $-2\log2$.</p>
<h3 id="tangent-on-kl--js-divergence">Tangent on KL &amp; JS Divergence<a hidden class="anchor" aria-hidden="true" href="#tangent-on-kl--js-divergence">#</a></h3>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://rdh1115.github.io/tags/generative-models/">Generative Models</a></li>
      <li><a href="https://rdh1115.github.io/tags/literature-review/">Literature Review</a></li>
    </ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>© 2024 Mark Bai</span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
